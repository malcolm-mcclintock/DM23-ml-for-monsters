{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47280f5d",
   "metadata": {},
   "source": [
    "# Word Embedding Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927f2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bs4 import *\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c78e8",
   "metadata": {},
   "source": [
    "We can mess with word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd2f32",
   "metadata": {},
   "source": [
    "# Option 1: Our embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68f194",
   "metadata": {},
   "source": [
    "We can use the embeddings that we trained! Or that we will train!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ea604",
   "metadata": {},
   "source": [
    "# Option 2: Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efda3d",
   "metadata": {},
   "source": [
    "Alternately, we can use pretrained vectors or embeddings downloaded from the internet. We can use Word2Vec, or GloVe, which is a model that came out a few years later and works very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc547a4",
   "metadata": {},
   "source": [
    "We use `gensim` which is a great library for working with embeddings and training topic models. It has some "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90503af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = api.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778d4bb",
   "metadata": {},
   "source": [
    "Print out available models (i.e. embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6786f264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors ...\n",
      "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state...\n",
      "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipe...\n",
      "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets,...\n",
      "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
      "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
      "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of...\n",
      "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors tra...\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(\n",
    "        '%s (%d records): %s' % (\n",
    "            model_name,\n",
    "            model_data.get('num_records', -1),\n",
    "            model_data['description'][:40] + '...',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12f52f",
   "metadata": {},
   "source": [
    "The first time you load a new dataset it will take a while to download, and the files can be quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c76360a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# keep this commented out - I'm using this dataset \n",
    "#model_news = api.load(\"word2vec-google-news-300\")\n",
    "# You should use these embeddings, and compare your results with mine!\n",
    "model_wiki =  api.load(\"glove-wiki-gigaword-50\")\n",
    "model_twitter =  api.load(\"glove-twitter-100\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b57b1",
   "metadata": {},
   "source": [
    "## Option 3: Train Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374a3f0",
   "metadata": {},
   "source": [
    "You can also train embeddings from a saved corpus. \n",
    "\n",
    " For our experiments, we're going to use the abstracts of all ArXiv papers in the category cs.CL (computation and language) that were published before mid-April 2021 â€” a total of around 25,000 documents. We tokenize these abstracts with spaCy.\n",
    "\n",
    "We define a wrapper to deal with csv data with one column.\n",
    "\n",
    "Each column contains an abstract of an NLP paper. \n",
    "\n",
    "If your csv looks different, you will have to change this function to get the right column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db35d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.filename, \"r\") as i:\n",
    "            reader = csv.reader(i, delimiter=\",\")\n",
    "            for _, abstract in reader:\n",
    "                tokens = [t.text.lower() for t in self.nlp(abstract)]\n",
    "                yield tokens\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "406c88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = Corpus(\"../data/arxiv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d6c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_model = gensim.models.Word2Vec(documents, min_count=50, window=5, vector_size=100)\n",
    "arxiv_model.save(\"word2vec.arxiv.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba82c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_model = arxiv_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d0653",
   "metadata": {},
   "source": [
    "When we train our word embeddings, gensim allows us to set a number of parameters. The most important of these are min_count, window, vector_size and sg\n",
    "\n",
    "* `min_count` is the minimum frequency of the words in our corpus. For infrequent words, we just don't have enough information to train reliable word embeddings. It therefore makes sense to set this minimum frequency to at least 10. In these experiments, we'll set it to 100 to limit the size of our model even more.\n",
    "* `window` is the number of words to the left and to the right that make up the context that word2vec will take into account.\n",
    "* `vector_size` is the dimensionality of the word vectors. This is generally between 100 and 1000. This dimensionality often forces us to make a trade-off: embeddings with a higher dimensionality are able to model more information, but also need more data to train.\n",
    "* `sg`: there are two algorithms to train word2vec: skip-gram and CBOW. Skip-gram tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. By default, Gensim uses CBOW (sg=0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ae13e",
   "metadata": {},
   "source": [
    "# Playing with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14175dfe",
   "metadata": {},
   "source": [
    "Let's take a look at the trained model. The word embeddings are on its wv attribute, and we can access them by the using the token as key. For example, here is the embedding for nlp, with the requested 100 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e670b",
   "metadata": {},
   "source": [
    "We can also easily find the similarity between two words. Similarity is measured as the cosine between the two word embeddings, and therefore ranges between -1 and +1. The higher the cosine, the more similar two words are. As expected, the figures below show that nmt (neural machine translation) is closer to smt (statistical machine translation) than to ner (named entity recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9a7b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to model = whatever vectors you have loaded to explore them in the code below\n",
    "\n",
    "model = model_twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6dd7ee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5545392\n",
      "0.4175607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('romney', 0.8368731737136841),\n",
       " ('potus', 0.7850850224494934),\n",
       " ('obama', 0.7849058508872986),\n",
       " ('clinton', 0.7623938322067261),\n",
       " ('barack', 0.7508866190910339),\n",
       " ('rnc', 0.7207996249198914),\n",
       " ('boehner', 0.7167794704437256),\n",
       " ('president', 0.6993654370307922),\n",
       " ('palin', 0.6886443495750427),\n",
       " ('debates', 0.6864507794380188)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(model.similarity(\"language\", \"logic\"))\n",
    "print(model.similarity(\"language\", \"play\"))\n",
    "\n",
    "model.most_similar(positive=['biden'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21891e8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In a similar vein, we can find the words that are most similar to a target word. The words with the most similar embedding to bert are all semantically related to it: other types of pretrained models such as roberta, mbert, xlm, as well as the more general model type BERT represents (transformer and transformers), and more generally related words (pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a2c9d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('promote', 0.7328717112541199),\n",
       " ('establishing', 0.7325128316879272),\n",
       " ('development', 0.7282374501228333),\n",
       " ('promoting', 0.7234703302383423),\n",
       " ('cooperation', 0.7193742990493774),\n",
       " ('partnership', 0.7100776433944702),\n",
       " ('sustainable', 0.7055977582931519),\n",
       " ('productive', 0.7023764848709106),\n",
       " ('develop', 0.7007005214691162),\n",
       " ('collaborative', 0.6980806589126587)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word(\"cooperative\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1212b",
   "metadata": {},
   "source": [
    "Interestingly, we can look for words that are similar to a set of words and dissimilar to another set of words at the same time. This allows us to look for analogies of the type BERT is to a transformer like an LSTM is to .... Our embedding model correctly predicts that LSTMs are a type of RNN, just like BERT is a particular type of transformer.\n",
    "\n",
    "    > solves\n",
    "    > 3 : 1 :: 2 : _____\n",
    "    > man : king :: woman : _______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfef183",
   "metadata": {},
   "source": [
    "#### Exercise: solve the analogy with your vectors!\n",
    "    \n",
    "    > pasta : pizza :: poutine : _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "847b6872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('poos', 0.6951389908790588),\n",
       " ('charr', 0.6802443265914917),\n",
       " ('milfoil', 0.6759202480316162),\n",
       " ('adnet', 0.6674808263778687),\n",
       " ('poliakov', 0.6673842668533325),\n",
       " ('kalyuzhny', 0.6571604013442993),\n",
       " ('dauphinois', 0.6463200449943542),\n",
       " ('delamontagne', 0.6451795697212219),\n",
       " ('vedrine', 0.6449277400970459),\n",
       " ('splenomegaly', 0.6439857482910156)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# work here\n",
    "#\n",
    "model.most_similar(positive=['poutine','pasta'],negative=['pizza'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861ebab",
   "metadata": {},
   "source": [
    "## Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0ce8c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Similarly, we can also zoom in on one of the meanings of ambiguous words. For example, in NLP tree has a very specific meaning, which is obvious from its nearest neighbours constituency, parse, dependency and syntax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"pizza\", \"poutine\"], negative=[\"pasta\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"tree\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa5364",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "However, if we specify we're looking for words that are similar to tree, but dissimilar to forest, suddenly it gives a different, more domesticated image of a tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c10c2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prosthetic', 0.6965978145599365),\n",
       " ('robotic', 0.67524653673172),\n",
       " ('reptilian', 0.6654907464981079),\n",
       " ('mind-control', 0.6636196970939636),\n",
       " ('cad/cam', 0.6501995921134949),\n",
       " ('prosthetics', 0.6475244760513306),\n",
       " ('exfoliating', 0.6458118557929993),\n",
       " ('prosthesis', 0.6454856395721436),\n",
       " ('constructs', 0.6423043608665466),\n",
       " ('cyborg', 0.6421464681625366)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"cybernetic\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4915ba6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Finally, we can present the word2vec model with a list of words and ask it to identify the odd one out. It then uses the word embeddings to identify the word that is least similar to the other ones. For example, in the list lstm cnn gru svm transformer, it correctly identifies svm as the only non-neural model. In the list bert word2vec gpt-2 roberta xlnet, it correctly singles out word2vec as the only non-transormer model. In word2vec bert glove fasttext elmo, bert is singled out as the only transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "985cbb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drab\n",
      "bureaucracy\n",
      "elmo\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"effervescent bubbly sparkling drab\".split()))\n",
    "print(model.doesnt_match(\"monarchy democracy bureaucracy communism socialism\".split()))\n",
    "print(model.doesnt_match(\"bert ernie elmo barney\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d100b2",
   "metadata": {},
   "source": [
    "# Plotting Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bf5d6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's now visualize some of our embeddings. To plot embeddings with a dimensionality of 100 or more, we first need to map them to a dimensionality of 2. We do this with the popular t-SNE method. T-SNE, short for t-distributed Stochastic Neighbor Embedding, helps us visualize high-dimensional data by mapping similar data to nearby points and dissimilar data to distance points in the low-dimensional space.\n",
    "\n",
    "T-SNE is present in Scikit-learn. To run it, we just have to specify the number of dimensions we'd like to map the data to (n_components), and the similarity metric that t-SNE should use to compute the similarity between two data points (metric). We're going to map to 2 dimensions and use the cosine as our similarity metric. Additionally, we use PCA as an initialization method to remove some noise and speed up computation. The Scikit-learn user guide contains some additional tips for optimizing performance.\n",
    "\n",
    "Plotting all the embeddings in our vector space would result in a very crowded figure where the labels are hardly legible. Therefore we'll focus on a subset of embeddings by selecting the 200 most similar words to a target word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list of words to map\n",
    "target_word = \"disaster\"\n",
    "selected_words = [w[0] for w in model.most_similar(positive=[target_word], topn=100)] + [target_word]\n",
    "\n",
    "# list of embeddings for our target words\n",
    "embeddings = [model[w] for w in selected_words] + model[\"bert\"]\n",
    "\n",
    "# 2-D reduction of embeddings\n",
    "mapped_embeddings = TSNE(n_components=2, metric='cosine', init='pca').fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c9f9d",
   "metadata": {},
   "source": [
    "\n",
    "If we take bert as our target word, the figure shows some interesting patterns. In the immediate vicinity of bert, we find the similar transformer models that we already identified as its nearest neighbours earlier: xlm, mbert, gpt-2, and so on. Other parts of the picture have equally informative clusters of NLP tasks and benchmarks (squad and glue), languages (german and english), neural-network architectures (lstm, gru, etc.), embedding types (word2vec, glove, fasttext, elmo), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "x = mapped_embeddings[:,0]\n",
    "y = mapped_embeddings[:,1]\n",
    "plt.scatter(x, y)\n",
    "\n",
    "\n",
    "# add labels to our map visualization\n",
    "for i, txt in enumerate(selected_words):\n",
    "    plt.annotate(txt, (x[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834f87a",
   "metadata": {},
   "source": [
    "# Compare to Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd85179c",
   "metadata": {},
   "source": [
    "Here are 50 dimension GloVe vectors from twitter. Use these for the exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c4b6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model and return as object ready for use\n",
    "# embeddings_glove_twitter = api.load(\"glove-twitter-25\")\n",
    "model_glove_twitter = model_twitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3b73c",
   "metadata": {},
   "source": [
    "## Exercise: Semantic Similarity\n",
    "\n",
    "\n",
    "What is the semantic similarity between 'meaning' and 'interpretation' in twitter space?\n",
    "What about meaning and extract?\n",
    "\n",
    "How does this compare to the arxiv embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2525dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here\n",
    "#\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1d497",
   "metadata": {},
   "source": [
    "Pick two more concepts and compare their cosine similarity in two different vector space models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4303c",
   "metadata": {},
   "source": [
    "## Exercise: Near Neighbors\n",
    "\n",
    "What are the nearest neighbors of these concepts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42296ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6366b",
   "metadata": {},
   "source": [
    "## Exercise: Analogy\n",
    "\n",
    "How does twitter solve the tree analogy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd171025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895402c7",
   "metadata": {},
   "source": [
    "Try another analogy. What is the result? In your mind, what shoudl the answer be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5571e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89147e32",
   "metadata": {},
   "source": [
    "# Exercise: Plot spatial relationships\n",
    "\n",
    "Pick a set of words that are related to each other in the same way. For instance you could use countries and their capitals, or adjectives and their superlatives\n",
    "\n",
    "e.g.\n",
    "\n",
    "```\n",
    "Rome - Italy\n",
    "\n",
    "Beijing - China\n",
    "\n",
    "Berlin - Germany\n",
    "\n",
    "Ottowa - Canada\n",
    "```\n",
    "\n",
    "superlatives\n",
    "``` \n",
    "bad - worse\n",
    "\n",
    "good - better\n",
    "\n",
    "warm - warmer\n",
    "\n",
    "red - redder\n",
    "\n",
    "blue - bluer\n",
    "```\n",
    "\n",
    "What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37299d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Answer here\n",
    "\n",
    "You'll need to copy the code for visualization from above \n",
    "but edit certain parts to give you the embeddings of the words \n",
    "you want to look at\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bbb6e",
   "metadata": {},
   "source": [
    "#  Training new embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4162f",
   "metadata": {},
   "source": [
    "Challenge: train a word2vec model on a corpus of your choosing or of your creation. Now that we have one line to do all of the work of yesterday, the challenge is getting the data into the right shape. The function\n",
    "\n",
    "`gensim.models.Word2Vec(documents, min_count=x, window=x, vector_size=x)`\n",
    "\n",
    "should be able to take any kind o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291dccf6",
   "metadata": {},
   "source": [
    "## Load your own corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3906e5",
   "metadata": {},
   "source": [
    "#### CSV Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce9d7fc",
   "metadata": {},
   "source": [
    "Load the corpus into memory (returns a Dataset object, which is the exact kind of object we need)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8a9c0",
   "metadata": {},
   "source": [
    "We define a wrapper to deal with csv data with one column.\n",
    "\n",
    "Each column contains an abstract of an NLP paper. \n",
    "\n",
    "If your csv looks different, you will have to change this function to get the right column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0678e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.filename, \"r\") as i:\n",
    "            reader = csv.reader(i, delimiter=\",\")\n",
    "            for _, abstract in reader:\n",
    "                tokens = [t.text.lower() for t in self.nlp(abstract)]\n",
    "                yield tokens\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4902e",
   "metadata": {},
   "source": [
    "Then you can create your corpus by initializing a Corpus object with the relative path to your csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b0b2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = Corpus(\"../data/arxiv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea0952",
   "metadata": {},
   "source": [
    "#### Load text from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66a3c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://theanarchistlibrary.org/library/the-invisible-committe-now.muse'\n",
    "res = requests.get(url)\n",
    "html_page = res.text\n",
    "\n",
    "# Parse the source code using BeautifulSoup\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "# Extract the plain text content\n",
    "text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc0f4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "        spaced_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', text) \n",
    "        self.sentences = spaced_corpus.split('.')\n",
    "\n",
    "        \n",
    "    def __iter__(self):\n",
    "            \n",
    "               \n",
    "        # separate punctuation from previous word\n",
    "        \n",
    "        # split into sentences\n",
    "        for sentence in self.sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45c0c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe118c2",
   "metadata": {},
   "source": [
    "train a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "592743ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_model = gensim.models.Word2Vec(corpus, min_count=0, window=5, vector_size=100)\n",
    "now_model = now_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84205c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17780584,  0.14165595, -0.02023686,  0.04731496, -0.03528563,\n",
       "       -0.41216475,  0.08436754,  0.5550741 , -0.29759452, -0.3302673 ,\n",
       "        0.0532984 , -0.3605916 ,  0.00634871,  0.19888034,  0.1136161 ,\n",
       "       -0.12106553,  0.09652579, -0.16625862, -0.08971754, -0.44926703,\n",
       "        0.16550542, -0.02957334,  0.27742964, -0.17005129,  0.05981685,\n",
       "        0.08914005, -0.2739902 ,  0.0081562 , -0.20762356,  0.10143223,\n",
       "        0.37559932, -0.09632047,  0.14442205, -0.27530015,  0.01123251,\n",
       "        0.2604527 ,  0.06122362, -0.05353915, -0.05280174, -0.30480772,\n",
       "        0.09729952, -0.23737392, -0.17903922, -0.00507795,  0.20530565,\n",
       "       -0.04261357, -0.13149163, -0.05699653,  0.17866972,  0.16170652,\n",
       "        0.20821849, -0.23083907,  0.01231476, -0.02665225, -0.04280896,\n",
       "        0.08481832,  0.1451759 ,  0.04481702, -0.0545764 ,  0.12536135,\n",
       "        0.00405   , -0.02745885,  0.00898578, -0.01171332, -0.26224548,\n",
       "        0.30893615, -0.01165136,  0.21606153, -0.2752248 ,  0.26015368,\n",
       "       -0.1674087 ,  0.11691974,  0.36580285,  0.01749094,  0.24912089,\n",
       "        0.08497558, -0.11480465, -0.02663199, -0.08534935, -0.03231914,\n",
       "       -0.2968266 , -0.08058393, -0.16118327,  0.3882161 , -0.13368994,\n",
       "       -0.11525103,  0.16184591,  0.0295424 ,  0.22715554,  0.12409101,\n",
       "        0.26180714, -0.04377994,  0.10942339, -0.04161735,  0.4441495 ,\n",
       "        0.15677457,  0.16963366, -0.15564008, -0.02422384, -0.14227417],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now_model[\"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637cf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
